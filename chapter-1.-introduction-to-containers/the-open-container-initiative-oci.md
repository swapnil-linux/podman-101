# The Open Container Initiative (OCI)

Following Docker's release, a large community emerged around the idea of using containers as the standard unit of software delivery. As companies started using containers to package and deploy their software more and more, Docker's container runtime did not meet all technical and business needs that engineering teams could have. In response to this, the community started developing new runtimes with different implementations and capabilities. Simultaneously, **new tools for building container images aimed to improve on Docker's speed or ease of use**. To make sure that all container runtimes could run images produced by any build tool, the community started the [Open Container Initiative](https://www.opencontainers.org/) — or **OCI** — to define industry standards around container image formats and runtimes.

Docker's original image format has become the [OCI Image Specification](https://github.com/opencontainers/image-spec), and various open-source build tools support it, including:

* [BuildKit](https://github.com/moby/buildkit), an optimized rewrite of Docker's build engine;
* [Podman](https://github.com/containers/libpod), an alternative implementation of Docker's command-line tool;
* [Buildah](https://github.com/containers/buildah), a command-line alternative to writing Dockerfiles;

Given an OCI image, any container runtime that implements the [OCI Runtime Specification](https://github.com/opencontainers/runtime-spec) **can unbundle the image and run its contents in an isolated environment**. Docker donated its runtime, [runc](https://github.com/opencontainers/runc), to the OCI to serve as the first implementation of the standard. Other open-source implementations exist, including:

* [Kata containers](https://katacontainers.io/), which **use virtual machines for improved isolation**. Docker’s use of Linux namespaces has some flaws which allow applications to escape their containers under certain circumstances. For specific use-cases, like running untrusted workloads, stronger security guarantees are required; Kata containers aim to make using VMs as simple as using Docker containers.
* [gVisor](https://gvisor.dev/), a.k.a runsc, which **focuses on security and efficiency**. Released in 2018 by Google, gVisor stands half-way between machine virtualization and Linux namespacing. It runs containerized applications inside a sandbox that implements many Linux system calls in userspace. In other words, applications running inside the gVisor sandbox rarely interact with the underlying Linux kernel directly, reducing the attack surface untrusted workloads may exploit. This approach allows for increased security while not incurring the performance cost of running a virtual machine.&#x20;
* [Firecracker](https://github.com/firecracker-microvm/firecracker), a **runtime optimized for serverless workloads**. This container technology powers [AWS Lambda](https://aws.amazon.com/lambda/) and [AWS Fargate](https://aws.amazon.com/fargate/). Building on the same virtualization techniques behind Google’s [Chrome OS](https://en.wikipedia.org/wiki/Chrome\_OS), Firecracker runs containerized applications inside MicroVMs: lightweight virtual machines optimized for running single applications instead of entire operating systems. This approach allows serverless computing providers to maximize the number of workloads they can run without compromising the isolation between their users’ programs.

Thanks to these open standards, the use of containers became widespread across the software industry, and new engineering solutions were needed to make practical use of containers at scale. One of these solutions was **Kubernetes**: a distributed platform for **orchestrating containers** on large clusters of machines. When Google released Kubernetes in 2015, the individual nodes of the cluster used Docker's runtime to run containers and manage container images. In late 2016, developers introduced an abstraction between Kubernetes and the container runtime it uses: the [Container Runtime Interface](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) — or CRI, for short.

To plug a new container runtime into Kubernetes, all that is needed is a small piece of code called a shim that translates requests made by Kubernetes into requests understandable by the runtime. In theory, each additional runtime would need a custom shim, but a generic one exists for all container runtimes that implement the OCI Runtime Specification. This shim is [CRI-O](https://cri-o.io/), another open-source project created by the community. When combined, these standards and abstractions have a powerful effect: **developers can ship any compliant container image to any compliant** Kubernetes cluster.
